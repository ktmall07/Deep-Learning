{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEwpv0GZ_CrT"
      },
      "source": [
        "<a \n",
        "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab10.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnoEhAVvBcMj"
      },
      "source": [
        "#Lab 9: Transfer Learning/Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBOvJdJfkXIL"
      },
      "source": [
        "## Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiuvTUWOjtBC"
      },
      "source": [
        "### Objective\n",
        "\n",
        "- Gain experience fine-tuning pre-trained models to domain-specific applications.\n",
        "\n",
        "### Deliverable\n",
        "\n",
        "For this lab you will submit an ipython notebook via learning suite. The bulk of the work is in modifying fine-tuning a pre-trained ResNet. Fine-tuning the GPT-2 language model is pretty easy. The provided code works as is; you will just have to swap in your own text dataset.\n",
        "\n",
        "### Grading\n",
        "\n",
        "- 35% Create a dataset class for your own dataset\n",
        "- 35% Create a network class that wraps a pretrained ResNet\n",
        "- 20% Implement unfreezing in the network class\n",
        "- 10% Fine-tune GPT-2 on your own dataset\n",
        "\n",
        "### Tips\n",
        "- Your life will be better if you download a dataset that already has the data in the expected format for ImageFolder (make sure to read the documentation!). The datasets recommended below are in the correct format.\n",
        "- Get the CNN working on the provided dataset (bird species classification) before swapping in your own.\n",
        "- For reference on freezing/unfreezing network weights, see [this github gist](https://gist.github.com/jcjohnson/6e41e8512c17eae5da50aebef3378a4c)\n",
        "- For training GPT-2, first try the medium-size (355M parameter) model. If your Colab instance doesn't have enough GPU space, you may need to switch to the small-size (124M parameter) model, but the results will be less impressive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dKzRORuLBNLR"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet152\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "import os\n",
        "import sys\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4R3D8Mr8b54"
      },
      "source": [
        "## 1 Fine-tune a ResNet for image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFoEeTYHDq2s"
      },
      "source": [
        "### 1.1 Find a dataset to fine-tune on, and make a Dataset class (1 hr.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z6g7a_Y84n0"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8NtFZRd5hcm"
      },
      "source": [
        "- Inherit from torch.utils.data.Dataset\n",
        "- Use a [torchvision.datasets.ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder)\n",
        "- Don't spend too long finding another dataset. Some suggestions that you are free to use:\n",
        " - https://www.kaggle.com/akash2907/bird-species-classification\n",
        " - https://www.kaggle.com/jessicali9530/stanford-dogs-dataset\n",
        " - https://www.kaggle.com/puneet6060/intel-image-classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBigIUFTukeJ"
      },
      "source": [
        "#### Help for downloading kaggle datasets\n",
        "Downloading Kaggle datasets requires authentication, so you can't just download from a url. Here are some step-by-step instructions of how to get Kaggle datasets in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X29UC6CvwfQ"
      },
      "source": [
        "1. Create an API key in Kaggle\n",
        "    - Click on profile photo\n",
        "    - Go to 'My Account'\n",
        "    - Scroll down to the API access section and click \"Create New API Token\"\n",
        "    - `kaggle.json` is now downloaded to your computer\n",
        "\n",
        "2. Upload the API key and install the Kaggle API client by running the next cell (run it again if it throws an error the first time). Also, `files.upload()` may not work in Firefox. One solution is to expand the Files banner (indicated by the '>' tab on the left side of the page) and use that to upload the key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "Mhjc0pM7jOoZ",
        "outputId": "91b38261-615b-467e-8c79-cde7b45b158a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-456ef37e-d515-48c8-8f00-d7f400bf53dd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-456ef37e-d515-48c8-8f00-d7f400bf53dd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "-rw-r--r-- 1 root root 67 Mar 13 22:17 kaggle.json\n"
          ]
        }
      ],
      "source": [
        "# Run this cell and select the kaggle.json file downloaded\n",
        "# from the Kaggle account settings page.\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "# Next, install the Kaggle API client.\n",
        "!pip install -q kaggle\n",
        "# Let's make sure the kaggle.json file is present.\n",
        "!ls -lha kaggle.json\n",
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGlIa4SIwEXB"
      },
      "source": [
        "3. Copy the desired dataset locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HtB-XdIr1EE",
        "outputId": "1ce40efa-fb09-413b-fa5f-d7c71619c4e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading bird-species-classification.zip to /content\n",
            " 99% 1.36G/1.37G [00:10<00:00, 132MB/s]\n",
            "100% 1.37G/1.37G [00:10<00:00, 136MB/s]\n",
            "Downloading stanford-dogs-dataset.zip to /content\n",
            "100% 748M/750M [00:05<00:00, 192MB/s]\n",
            "100% 750M/750M [00:05<00:00, 137MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Example download command for dataset found here: https://www.kaggle.com/akash2907/bird-species-classification\n",
        "!kaggle datasets download -d akash2907/bird-species-classification\n",
        "!kaggle datasets download -d jessicali9530/stanford-dogs-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcz0JGXjxFGe"
      },
      "source": [
        "#### Make the Dataset class\n",
        "See the implementation below for reference, and make your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lthPlsGeK4CX",
        "outputId": "57b991fa-0a23-4db6-d209-d1b5677bfaa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./bird-species-classification.zip\n",
            "Resizing images\n"
          ]
        }
      ],
      "source": [
        "class BirdDataset(Dataset):\n",
        "    def __init__(self, zip_file='./bird-species-classification.zip', size=256, train=True, upload=False):\n",
        "        super(BirdDataset, self).__init__()\n",
        "        \n",
        "        self.train = train\n",
        "        extract_dir = os.path.splitext(zip_file)[0]\n",
        "        if not os.path.exists(extract_dir):\n",
        "            os.makedirs(extract_dir)\n",
        "            self.extract_zip(zip_file, extract_dir)\n",
        "            # Resize the images - originally they are high resolution. We could do this\n",
        "            # in the DataLoader, but it will read the full-resolution files from disk\n",
        "            # every time before resizing them, making training slow\n",
        "            self.resize(extract_dir, size=size)\n",
        "\n",
        "        postfix = 'train' if train else 'test'\n",
        "            \n",
        "        if train:\n",
        "            # The bird-species dataset mistakenly has a train_data folder inside of train_data\n",
        "            self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'train_data', 'train_data'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "        else:\n",
        "            self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'test_data', 'test_data'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "    def extract_zip(self, zip_file, extract_dir):\n",
        "        print(\"Extracting\", zip_file)\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "\n",
        "    def resize(self, path, size=256):\n",
        "        \"\"\"Resizes all images in place\"\"\"\n",
        "        print(\"Resizing images\")\n",
        "        dirs = os.walk(path)\n",
        "        for root, dirs, files in os.walk(path):\n",
        "            for item in files:\n",
        "                name = os.path.join(root, item)\n",
        "                if os.path.isfile(name):\n",
        "                    im = Image.open(name)\n",
        "                    im = ImageOps.fit(im, (size, size))\n",
        "                    im.save(name[:-3] + 'bmp', 'BMP')\n",
        "                    os.remove(name)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.dataset_folder[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset_folder)\n",
        "\n",
        "bird_data = BirdDataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5jHFdToeDtIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f63030d4-bc32-4bb3-8013-1f4917a3d386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./stanford-dogs-dataset.zip\n",
            "Resizing images\n"
          ]
        }
      ],
      "source": [
        "#########################\n",
        "# Implement your own Dataset\n",
        "#########################\n",
        "class DogDataset(Dataset):\n",
        "    def __init__(self, zip_file='./stanford-dogs-dataset.zip', size=256, train=True, upload=False):\n",
        "\n",
        "        super(DogDataset, self).__init__()\n",
        "\n",
        "        self.train = train\n",
        "        extract_dir = os.path.splitext(zip_file)[0]\n",
        "        if not os.path.exists(extract_dir):\n",
        "            os.makedirs(extract_dir)\n",
        "            self.extract_zip(zip_file, extract_dir)\n",
        "            # Resize the images - originally they are high resolution. We could do this\n",
        "            # in the DataLoader, but it will read the full-resolution files from disk\n",
        "            # every time before resizing them, making training slow\n",
        "            self.resize(extract_dir, size=size)\n",
        "\n",
        "        # postfix = 'train' if train else 'test'\n",
        "\n",
        "        # if train:\n",
        "        #     # The bird-species dataset mistakenly has a train_data folder inside of train_data\n",
        "        #     self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'train_data', 'train_data'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "        # else:\n",
        "        #     self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'test_data', 'test_data'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "        self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'images'))\n",
        "\n",
        "    def extract_zip(self, zip_file, extract_dir):\n",
        "        print(\"Extracting\", zip_file)\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "\n",
        "    def resize(self, path, size=256):\n",
        "        \"\"\"Resizes all images in place\"\"\"\n",
        "        print(\"Resizing images\")\n",
        "        dirs = os.walk(path)\n",
        "        for root, dirs, files in os.walk(path):\n",
        "            for item in files:\n",
        "                name = os.path.join(root, item)\n",
        "                name = name + '.jpg'\n",
        "                if os.path.isfile(name):\n",
        "                    im = Image.open(name)\n",
        "                    im = ImageOps.fit(im, (size, size))\n",
        "                    im.save(name[:-3] + 'bmp', 'BMP')\n",
        "                    os.remove(name)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.dataset_folder[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset_folder)\n",
        "\n",
        "dog_data = DogDataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vJVbYcAJAf2"
      },
      "source": [
        "### 1.2 Wrap a pretrained ResNet in an `nn.Module` (30 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMOzGDND9FD1"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLvmDHbl9IyG"
      },
      "source": [
        "- Make a model class that inherits from `nn.Module`\n",
        "- Wrap a pretrained ResNet and swap out the last layer of that network with a layer that maps to the number of classes in your new dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOtl8z8G9wbr"
      },
      "source": [
        "#### Make your model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AY-XU4Mwas0j"
      },
      "outputs": [],
      "source": [
        "class ResNetBirds(nn.Module):\n",
        "    def __init__(self, num_classes, start_frozen=False):\n",
        "        super(ResNetBirds, self).__init__()\n",
        "\n",
        "        # Part 1.2\n",
        "        # Load the model - make sure it is pre-trained\n",
        "        self.resnet = resnet152(pretrained=True)\n",
        "\n",
        "        # Part 1.4\n",
        "        if start_frozen:\n",
        "            # Turn off all gradients of the resnet\n",
        "            self.resnet.requires_grad = False\n",
        "        \n",
        "        # Part 1.2\n",
        "        # Look at the code of torchvision.models.resnet152 to find the name of the attribute to override (the last layer of the resnet)\n",
        "        # Override the last layer of the neural network to map to the correct number of classes. Note that this new layer has requires_grad = True\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "        \n",
        "    def unfreeze(self, n_layers):\n",
        "        # Part 1.4\n",
        "        # Turn on gradients for the last n_layers\n",
        "        for p in list(reversed(list(self.resnet.parameters())))[:n_layers]:\n",
        "          p.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Part 1.2\n",
        "        # Pass x through the resnet\n",
        "        return self.resnet(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetDogs(nn.Module):\n",
        "    def __init__(self, num_classes, start_frozen=False):\n",
        "        super(ResNetDogs, self).__init__()\n",
        "\n",
        "        # Part 1.2\n",
        "        # Load the model - make sure it is pre-trained\n",
        "        self.resnet = resnet152(pretrained=True)\n",
        "\n",
        "        # Part 1.4\n",
        "        if start_frozen:\n",
        "            # Turn off all gradients of the resnet\n",
        "            self.resnet.requires_grad = False\n",
        "        \n",
        "        # Part 1.2\n",
        "        # Look at the code of torchvision.models.resnet152 to find the name of the attribute to override (the last layer of the resnet)\n",
        "        # Override the last layer of the neural network to map to the correct number of classes. Note that this new layer has requires_grad = True\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
        "        \n",
        "    def unfreeze(self, n_layers):\n",
        "        # Part 1.4\n",
        "        # Turn on gradients for the last n_layers\n",
        "        for p in list(reversed(list(self.resnet.parameters())))[:n_layers]:\n",
        "          p.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Part 1.2\n",
        "        # Pass x through the resnet\n",
        "        return self.resnet(x)"
      ],
      "metadata": {
        "id": "0Rr_AwxCOtuD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Krh0eYy18R9"
      },
      "source": [
        "### 1.3 Read through and run this training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yOGrrw2gbIPf"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_hat, y_truth):\n",
        "    \"\"\"Gets average accuracy of a vector of predictions\"\"\"\n",
        "    \n",
        "    preds = torch.argmax(y_hat, dim=1)\n",
        "    acc = torch.mean((preds == y_truth).float())\n",
        "    return acc\n",
        "\n",
        "def evaluate(model, objective, val_loader, device):\n",
        "    \"\"\"Gets average accuracy and loss for the validation set\"\"\"\n",
        "\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "    # model.eval() so that batchnorm and dropout work in eval mode\n",
        "    model.eval()\n",
        "    # torch.no_grad() to turn off computation graph creation. This allows for temporal\n",
        "    # and spatial complexity improvements, which allows for larger validation batch \n",
        "    # sizes so it’s recommended\n",
        "    with torch.no_grad():\n",
        "        for x, y_truth in val_loader:\n",
        "\n",
        "            x, y_truth = x.to(device), y_truth.to(device)\n",
        "            y_hat = model(x)\n",
        "            val_loss = objective(y_hat, y_truth)\n",
        "            val_acc = accuracy(y_hat, y_truth)\n",
        "\n",
        "            val_losses.append(val_loss.item())\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return torch.mean(torch.Tensor(val_losses)), torch.mean(torch.Tensor(val_accs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VKESMcKi2E_f"
      },
      "outputs": [],
      "source": [
        "def train(start_frozen=False, model_unfreeze=0):\n",
        "    \"\"\"Fine-tunes a CNN\n",
        "    Args:\n",
        "        start_frozen (bool): whether to start with the network weights frozen.\n",
        "        model_unfreeze (int): the maximum number of network layers to unfreeze\n",
        "    \"\"\"\n",
        "    epochs = 20\n",
        "    # Start with a very low learning rate\n",
        "    lr = .00005\n",
        "    val_every = 3\n",
        "    num_classes = 16\n",
        "    batch_size = 32\n",
        "    device = torch.device('cuda:0')\n",
        "\n",
        "    # Data\n",
        "    train_dataset = BirdDataset(upload=True, train=True)\n",
        "    val_dataset = BirdDataset(upload=True, train=False)\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              shuffle=True,\n",
        "                              num_workers=8,\n",
        "                              batch_size=batch_size)\n",
        "    val_loader = DataLoader(val_dataset,\n",
        "                              shuffle=True,\n",
        "                              num_workers=8,\n",
        "                              batch_size=batch_size)\n",
        "    \n",
        "    # Model\n",
        "    model = ResNetDogs(num_classes, start_frozen=start_frozen).to(device)\n",
        "    \n",
        "    # Objective\n",
        "    objective = nn.CrossEntropyLoss()\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-1)\n",
        "\n",
        "    # Progress bar\n",
        "    pbar = tqdm(total=len(train_loader) * epochs)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "    \n",
        "    cnt = 0\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Implement model unfreezing\n",
        "        if epoch < model_unfreeze:\n",
        "            # Part 1.4\n",
        "            # Unfreeze the last layers, one more each epoch\n",
        "            model.unfreeze(epochs)\n",
        "        \n",
        "        for x, y_truth in train_loader:\n",
        "        \n",
        "            x, y_truth = x.to(device), y_truth.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_hat = model(x)\n",
        "            train_loss = objective(y_hat, y_truth)\n",
        "            train_acc = accuracy(y_hat, y_truth)\n",
        "\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_accs.append(train_acc)\n",
        "            train_losses.append(train_loss.item())\n",
        "\n",
        "            if cnt % val_every == 0:\n",
        "                val_loss, val_acc = evaluate(model, objective, val_loader, device)\n",
        "                val_losses.append(val_loss)\n",
        "                val_accs.append(val_acc)\n",
        "\n",
        "            pbar.set_description('train loss:{:.4f}, train accuracy:{:.4f}.'.format(train_loss.item(), train_acc))\n",
        "            pbar.update(1)\n",
        "            cnt += 1\n",
        "\n",
        "    pbar.close()\n",
        "    plt.subplot(121)\n",
        "    plt.plot(np.arange(len(train_accs)), train_accs, label='Train Accuracy')\n",
        "    plt.plot(np.arange(len(val_accs) * val_every, step=val_every), val_accs, label='Val Accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(122)\n",
        "    plt.plot(np.arange(len(train_losses)), train_losses, label='Train Loss')\n",
        "    plt.plot(np.arange(len(val_losses) * val_every, step=val_every), val_losses, label='Val Loss')\n",
        "    plt.legend()\n",
        "    plt.show()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fvnxeLotchiH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719,
          "referenced_widgets": [
            "4f62d8fdf00849b793d430b3abdd56cf",
            "99c9608a5d9d40c2a9405a225f192276",
            "0018ecf6e3944c849448eed708ecd33a",
            "350e3f62f3694e4d9869b3fb4b05ad30",
            "f6b7662ab3a046839ff4499406111951",
            "527dcc8dd61a48ae8a07fb38508eef41",
            "71f5a369b5da47af890276f5a8d9fa3b",
            "0a0faee4971d4a50a5c859cf719f7269",
            "ada8ef9387a740bdb22e46acb3b2647e",
            "44af057aaf5245ed8848f2364e916049",
            "f6f5cb77c4504f2f89b7e2333395c28b"
          ]
        },
        "outputId": "1a4e7834-0afc-4482-f5bb-785b58b6a6e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/230M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f62d8fdf00849b793d430b3abdd56cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train loss:0.0798, train accuracy:1.0000.: 100%|██████████| 100/100 [06:09<00:00,  3.69s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-cdb687c0df6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_frozen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_unfreeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-6b567e4777b7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(start_frozen, model_unfreeze)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_accs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Val Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     '''\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "train(start_frozen=False, model_unfreeze=0)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEDv_-H7BvM0"
      },
      "source": [
        "### 1.4 Implement Unfreezing (1 hr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH5mQBaa-_0b"
      },
      "source": [
        "#### Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_YmE1pe-6LF"
      },
      "source": [
        "Unfreezing is a technique that can be helpful when fine tuning a CNN for a more difficult task with a large amount of data.\n",
        "\n",
        "The idea is that if we allow the network to tweak the earliest layers immediately, before the last FCL has been trained at all, the earliest layers will forget all of the useful features that they learned in order  to provide features that are helpful for the (untrained) FCL.\n",
        "\n",
        "So, rather than training all of the model weights at once, we learn the last fully connected layer, then train that layer together with the second-to-last layer, gradually adding layers until we reach the first layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMKRI77_-8nc"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaUc8BTYC1bz"
      },
      "source": [
        "- Modify your model class by setting the `requires_grad` attribute of the ResNet to `False`. (but keep `requires_grad = True` for the last layer).\n",
        "- Add a member function to you model class that allows the user to unfreeze weights in the training loop. See [this github gist](https://gist.github.com/jcjohnson/6e41e8512c17eae5da50aebef3378a4c) for reference.\n",
        "- Modify your training loop to add logic that calls the `unfreeze` function of the model class (unfreeze one layer every epoch).\n",
        "- Call your train function to fine-tune the ResNet on your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBT5jgifC7Im"
      },
      "source": [
        "#### Call your train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Mg9ySEO_BNDx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "9e63a3d6-2783-43ea-c328-5ce0fc0774eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "train loss:0.1076, train accuracy:1.0000.: 100%|██████████| 100/100 [06:14<00:00,  3.74s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e42e86f8922b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# train with unfreezing here (should be a single call to your train function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m############################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_frozen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_unfreeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-6b567e4777b7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(start_frozen, model_unfreeze)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_accs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Val Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     '''\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "############################\n",
        "# train with unfreezing here (should be a single call to your train function)\n",
        "############################\n",
        "train(start_frozen=True, model_unfreeze=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZItH2lX7k4Yt"
      },
      "source": [
        "You may not see any improvement for your classification task, but unfreezing can help convergence for more difficult image classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAXHAUf3EEiE"
      },
      "source": [
        "##2 Fine-tune a language model - (15 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu9usOxtjFHL"
      },
      "source": [
        "In this section you will use the gpt-2-simple package [here](https://github.com/minimaxir/gpt-2-simple) to fine-tune the GPT-2 language model on a domain of your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K7F19SPQo6U"
      },
      "source": [
        "### 2.1 Generate text from the pretrained GPT-2 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YLXvK51RnuL"
      },
      "source": [
        "#### Run this code to generate text from a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDNOb_H5IRvH",
        "outputId": "628648e9-379d-41e4-9a4d-8844cccb4eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpt-2-simple\n",
            "  Downloading gpt_2_simple-0.8.1.tar.gz (26 kB)\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (2.8.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (4.63.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gpt-2-simple) (1.21.5)\n",
            "Collecting toposort\n",
            "  Downloading toposort-1.7-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.24.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.13.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.44.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.5.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (57.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (13.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.1.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.8.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.10.0.2)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.5.1->gpt-2-simple) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gpt-2-simple) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.5.1->gpt-2-simple) (3.2.0)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.8.1-py3-none-any.whl size=24576 sha256=ce1407499662ba0bcf02cf9a05c769a1a6072cbee9da1fdc6956d4c42bbace23\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/89/8a/f5de6944286d1ac2658b0caa7eae3c8cda50f770cdc957217f\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: tf-estimator-nightly, toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.8.1 tf-estimator-nightly-2.8.0.dev2021122109 toposort-1.7\n",
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install gpt-2-simple\n",
        "\n",
        "# the transformers package is built on top of Tensorflow, and the default TF version \n",
        "# for Colab will soon switch to 2.x. We remedy this with the following magic method\n",
        "%tensorflow_version 1.x \n",
        "\n",
        "import gpt_2_simple as gpt2\n",
        "import os\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aRJ-c9uRMOa",
        "outputId": "16f35052-0796-448d-990c-ba3a851828e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 124M model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 443Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 4.36Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 376Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:11, 42.9Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 110Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 6.05Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 6.76Mit/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained model models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
            "The North Korean leader and his Korean counterpart, Kim Jong Un, are now in a frosty, uncertain relationship.\n",
            "\n",
            "If his threats to strike Seoul and the South are not enough to provoke Washington to take concrete action, President Trump may also want to make a tough choice: either cut off North Korea's nuclear capability, strike the North's weapons facilities, or strike him directly, in the case of North Korea itself.\n",
            "\n",
            "The most likely outcome is that Washington would go to war with Pyongyang, but perhaps not with North Korea itself. President Trump has said he has no interest in striking Pyongyang on a regional basis.\n",
            "\n",
            "North Korea has said it plans to use its nuclear weapons in a series of nuclear tests.\n",
            "\n",
            "Obama could offer an alternative to a military strike on North Korea, although he's less likely than Kim to do so. In theory, North Korea could try to use its nuclear arsenal to launch a missile or missile attack.\n",
            "\n",
            "But if Trump chooses to strike Pyongyang directly, he will face a difficult choice — one that could put him in a position where he has little interest in other countries.\n",
            "\n",
            "As a national security adviser, Trump has stressed that if he was to strike North Korea directly, it would be a major mistake.\n",
            "\n",
            "\"If we found a way out of this, I would go immediately to North Korea, because that's the path we would take,\" Trump told NBC's Chuck Todd on Sunday.\n",
            "\n",
            "Trump has already taken steps to curb North Korea's nuclear ambitions. He agreed to block the sale of U.S. military hardware to the country to help it develop its nuclear weapons, among other things, and he has also said he would look to provide North Korea with nuclear fuel.\n",
            "\n",
            "Even as Trump has stood by his national security advisers, Pyongyang has been characteristically silent on the possibility of war.\n",
            "\n",
            "\"I think that's very dangerous,\" said David Ignatius, a former senior adviser to President George W. Bush. \"He has been very clear about a very clear desire to pull out. He is talking about war, and I think that is very dangerous.\"\n",
            "\n",
            "Currently, Trump has no formal plans to strike North Korea in the near future. But in the months ahead, he might consider striking it, including in a series of nuclear tests.<|endoftext|>The US has so far made no progress in its attempts to access the files of Snowden, the former NSA contractor, who leaked documents to the media on the US government's massive surveillance programs.\n",
            "\n",
            "The US decided to use the documents to develop its own surveillance regime, which it calls \"the PRISM program\".\n",
            "\n",
            "\"The PRISM program is what's called the list of people who have committed serious crimes and crimes against humanity by the US government,\" a senior US official told the Guardian.\n",
            "\n",
            "\"It is not a list of people who have committed serious crimes against humanity, but it is what it is and what it has been for the last two years.\"\n",
            "\n",
            "The new US policy is being seen by some as a way of getting back to the US's long-running insistence that it has no right to see what it has to say about the suspects.\n",
            "\n",
            "It has been reported that Snowden has been granted asylum in Russia.\n",
            "\n",
            "On the surface, the US government believes that the information it has gathered about the people it has gathered as part of its PRISM program is valuable and valuable to the US.\n",
            "\n",
            "But there is some evidence that it could be vulnerable because the Americans have not been able to access the files of the people they are targets of surveillance.\n",
            "\n",
            "The US government says it has made it clear that it does not share the information with any foreign national.\n",
            "\n",
            "This contradicts the US government's statements that it has no right to share information with the Russians.\n",
            "\n",
            "According to the Guardian, it is believed that tax authorities in the US have been gathering information about the Russian government's various electronic communications.\n",
            "\n",
            "And it is out of court.\n",
            "\n",
            "The US government has been in a bind for years. It wanted to share with the Russians as many documents as possible that could be opened over the line of control by the US government. But that was always a long shot.\n",
            "\n",
            "The US government has been fighting to end the secrecy surrounding its PRISM program for years. It has spent almost a year trying to get out of court to stop the government from releasing the data.\n",
            "\n",
            "The US government claims it has never been able to fully disclose the data it has been gathering, and it would have to also have to go through the NSA to gain access.\n",
            "\n",
            "If the US government were to disclose the data to the Russians, it would be a major blow to the US government's ability to defend itself.\n",
            "\n",
            "But if the US government disclosed the data to the Russians, it would leave the US government with a lot more to go through to prove that it is not already in the dark about what the US government has been doing.\n",
            "\n",
            "It is also clear that\n"
          ]
        }
      ],
      "source": [
        "# This line is necessary to be able to run a new tf session\n",
        "tf.reset_default_graph()\n",
        "# The medium-sized model. IF you run out of memory, try \"124M\" instead\n",
        "model_name = \"124M\"\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "\tprint(f\"Downloading {model_name} model...\")\n",
        "\tgpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, model_name=model_name)\n",
        "gpt2.generate(sess, model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHmjSVf_FNHv"
      },
      "source": [
        "### 2.2 Download a text dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPXJkNubFyY6"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWkuRjbcFzwb"
      },
      "source": [
        "- Use the provided functions to download your own text dataset\n",
        "- [Project Gutenberg](https://www.gutenberg.org/) is a nice starting point for raw text corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD45m3IwF9hh"
      },
      "source": [
        "#### Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "a3735ee1f1454edca87121ad2e7eda75",
            "028e8c2bfc1b4c118634e62625601281",
            "e6b7cd670e574bd38b29ae4056fcf56d",
            "860f7787a5214a8c8c92ac581057c80b",
            "38e612eed865438abf8c74e7351809d1",
            "e57071f10ffb4b60afe794a3b74a01c9",
            "ea0fddb5e4184dfb9cf70922226abd54",
            "6b48f51dc73f422a8b9c675716d06ca5",
            "d04fc0fcb9a242ea8355c6c8700c96b2",
            "d1276c8b9ab7466abde92ae10e422e22",
            "a593cac4322f4015a8095a6d04c5bd36"
          ]
        },
        "id": "ESltl2QM5nxw",
        "outputId": "1edfc460-d058-49b0-9fdc-f57b91089a7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.gutenberg.org/files/5200/5200-0.zip to ../data/text.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/51498 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3735ee1f1454edca87121ad2e7eda75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/text.zip\n",
            "mv: cannot stat '/data/text/30.txt': No such file or directory\n",
            "5200-0.txt\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from torchvision import datasets\n",
        "\n",
        "def extract_zip(zip_path, remove_finished=True):\n",
        "    print('Extracting {}'.format(zip_path))\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(zip_path.replace('.zip', ''))\n",
        "    if remove_finished:\n",
        "        os.remove(zip_path)\n",
        "\n",
        "def download_dataset(url, root='../data'):\n",
        "    if not os.path.exists(os.path.join(root, 'text')):\n",
        "        os.makedirs(os.path.join(root))\n",
        "        datasets.utils.download_url(url, root, 'text.zip', None)\n",
        "        extract_zip(os.path.join(root, 'text.zip'))\n",
        "    return os.path.join(root, 'text')\n",
        "\n",
        "##########################################\n",
        "# Set the url for your dataset here,\n",
        "# move the dataset to the desired location\n",
        "##########################################\n",
        "url = 'https://www.gutenberg.org/files/5200/5200-0.zip'\n",
        "download_dataset(url)\n",
        "!mv /data/text/30.txt /data/text/bible.txt\n",
        "!ls ../data/text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usQE-rSPZq_X"
      },
      "source": [
        "### 2.3 Fine-tune GPT-2 on your own dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoA0tZZCa_1k"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoU6ML1mbgjP"
      },
      "source": [
        "- Swap out the dataset parameter with the path to your dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pa5vFJ5EUjv"
      },
      "source": [
        "#### Train on your dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuQ5snl4LuS0",
        "outputId": "f187cc5e-5718-43f4-b3c1-c92197b124b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 33746 tokens\n",
            "Training...\n",
            "[1 | 12.04] loss=3.44 avg=3.44\n",
            "[2 | 16.35] loss=3.41 avg=3.43\n",
            "[3 | 20.66] loss=2.90 avg=3.25\n",
            "[4 | 24.98] loss=2.86 avg=3.15\n",
            "[5 | 29.31] loss=3.23 avg=3.17\n",
            "[6 | 33.63] loss=3.09 avg=3.15\n",
            "[7 | 37.96] loss=3.10 avg=3.14\n",
            "[8 | 42.29] loss=2.72 avg=3.09\n",
            "[9 | 46.63] loss=3.23 avg=3.11\n",
            "[10 | 50.96] loss=2.98 avg=3.09\n",
            "[11 | 55.29] loss=2.88 avg=3.07\n",
            "[12 | 59.63] loss=2.97 avg=3.06\n",
            "[13 | 63.96] loss=2.80 avg=3.04\n",
            "[14 | 68.32] loss=2.26 avg=2.98\n",
            "[15 | 72.67] loss=2.85 avg=2.97\n",
            "[16 | 77.03] loss=2.73 avg=2.96\n",
            "[17 | 81.39] loss=2.39 avg=2.92\n",
            "[18 | 85.75] loss=1.39 avg=2.83\n",
            "[19 | 90.12] loss=2.63 avg=2.82\n",
            "[20 | 94.49] loss=2.47 avg=2.80\n",
            "[21 | 98.85] loss=1.80 avg=2.74\n",
            "[22 | 103.22] loss=2.54 avg=2.73\n",
            "[23 | 107.58] loss=2.30 avg=2.71\n",
            "[24 | 111.95] loss=2.57 avg=2.71\n",
            "[25 | 116.33] loss=2.57 avg=2.70\n",
            "[26 | 120.70] loss=2.58 avg=2.69\n",
            "[27 | 125.07] loss=2.46 avg=2.68\n",
            "[28 | 129.43] loss=2.68 avg=2.68\n",
            "[29 | 133.81] loss=1.58 avg=2.64\n",
            "[30 | 138.18] loss=1.97 avg=2.62\n",
            "[31 | 142.55] loss=2.17 avg=2.60\n",
            "[32 | 146.93] loss=2.15 avg=2.58\n",
            "[33 | 151.29] loss=2.56 avg=2.58\n",
            "[34 | 155.66] loss=2.47 avg=2.58\n",
            "[35 | 160.04] loss=2.33 avg=2.57\n",
            "[36 | 164.42] loss=1.95 avg=2.55\n",
            "[37 | 168.79] loss=2.34 avg=2.54\n",
            "[38 | 173.17] loss=2.00 avg=2.53\n",
            "[39 | 177.54] loss=1.92 avg=2.51\n",
            "[40 | 181.92] loss=2.04 avg=2.49\n",
            "[41 | 186.30] loss=2.42 avg=2.49\n",
            "[42 | 190.69] loss=2.20 avg=2.48\n",
            "[43 | 195.07] loss=1.50 avg=2.45\n",
            "[44 | 199.44] loss=2.02 avg=2.44\n",
            "[45 | 203.81] loss=2.16 avg=2.43\n",
            "[46 | 208.20] loss=1.85 avg=2.42\n",
            "[47 | 212.57] loss=1.99 avg=2.41\n",
            "[48 | 216.94] loss=2.18 avg=2.40\n",
            "[49 | 221.31] loss=2.14 avg=2.39\n",
            "[50 | 225.67] loss=1.58 avg=2.37\n",
            "[51 | 230.05] loss=1.60 avg=2.35\n",
            "[52 | 234.42] loss=1.48 avg=2.33\n",
            "[53 | 238.79] loss=1.47 avg=2.31\n",
            "[54 | 243.16] loss=1.92 avg=2.30\n",
            "[55 | 247.52] loss=1.80 avg=2.29\n",
            "[56 | 251.91] loss=2.03 avg=2.28\n",
            "[57 | 256.28] loss=1.95 avg=2.28\n",
            "[58 | 260.65] loss=1.60 avg=2.26\n",
            "[59 | 265.04] loss=1.25 avg=2.24\n",
            "[60 | 269.40] loss=1.42 avg=2.22\n",
            "[61 | 273.78] loss=1.75 avg=2.21\n",
            "[62 | 278.16] loss=1.95 avg=2.20\n",
            "[63 | 282.54] loss=1.78 avg=2.20\n",
            "[64 | 286.90] loss=1.57 avg=2.18\n",
            "[65 | 291.29] loss=1.53 avg=2.17\n",
            "[66 | 295.67] loss=1.80 avg=2.16\n",
            "[67 | 300.05] loss=1.87 avg=2.16\n",
            "[68 | 304.42] loss=1.17 avg=2.14\n",
            "[69 | 308.80] loss=1.35 avg=2.12\n",
            "[70 | 313.18] loss=0.95 avg=2.10\n",
            "[71 | 317.56] loss=1.61 avg=2.09\n",
            "[72 | 321.93] loss=0.78 avg=2.06\n",
            "[73 | 326.31] loss=1.49 avg=2.05\n",
            "[74 | 330.70] loss=1.21 avg=2.03\n",
            "[75 | 335.08] loss=1.15 avg=2.02\n",
            "[76 | 339.46] loss=1.33 avg=2.01\n",
            "[77 | 343.84] loss=0.81 avg=1.98\n",
            "[78 | 348.19] loss=1.01 avg=1.97\n",
            "[79 | 352.55] loss=0.64 avg=1.94\n",
            "[80 | 356.92] loss=1.25 avg=1.93\n",
            "[81 | 361.29] loss=1.18 avg=1.92\n",
            "[82 | 365.66] loss=0.92 avg=1.90\n",
            "[83 | 370.02] loss=0.93 avg=1.88\n",
            "[84 | 374.38] loss=0.77 avg=1.86\n",
            "[85 | 378.75] loss=1.34 avg=1.85\n",
            "[86 | 383.12] loss=1.04 avg=1.84\n",
            "[87 | 387.49] loss=0.84 avg=1.82\n",
            "[88 | 391.84] loss=0.77 avg=1.80\n",
            "[89 | 396.20] loss=0.82 avg=1.79\n",
            "[90 | 400.58] loss=1.15 avg=1.78\n",
            "[91 | 404.93] loss=1.14 avg=1.76\n",
            "[92 | 409.29] loss=0.64 avg=1.75\n",
            "[93 | 413.65] loss=0.72 avg=1.73\n",
            "[94 | 418.03] loss=0.76 avg=1.71\n",
            "[95 | 422.40] loss=0.67 avg=1.70\n",
            "[96 | 426.78] loss=0.72 avg=1.68\n",
            "[97 | 431.15] loss=0.57 avg=1.66\n",
            "[98 | 435.52] loss=0.72 avg=1.65\n",
            "[99 | 439.88] loss=0.53 avg=1.63\n",
            "[100 | 444.25] loss=0.89 avg=1.62\n",
            "======== SAMPLE 1 ========\n",
            ";\n",
            "but nonetheless I felt a slight warmth flow down my back and under my skirts. It was probably\n",
            "an electric tooth, electric; a gentle tug at the carpet spread it\n",
            "aggressively about my back, cheek, and underarm; a loud thump\n",
            "against my chin.\n",
            "\n",
            "I felt a mild, dull pain strike my back which I did\n",
            "regret slightly, but which I felt very carefully prepared to inflict\n",
            "with such precision. I turned my head to face the door as if it\n",
            "should be open, as I did not dare turn my back to the stairwell; I\n",
            "had already lost count of the number of times I had fallen asleep as the\n",
            "room spread itself out about my back, under my arms, and under my\n",
            "carpet. Hardly had any of the pain subsided as I lay\n",
            "nourished and nothing to eat had been left unchanged. As I\n",
            "remained where I was and lay quietly by the door, listening to\n",
            "the cleaner's enquiries as she went about her business she\n",
            "could not be seen entering the room. Instead she\n",
            "stretched her right hand out from under her skirt to her left and\n",
            "wristed slightly as if wishing people outside the flat aware\n",
            "of what she was doing. One time, for instance, she briefly\n",
            "stretched her right hand out from under her skirt to her left as a\n",
            "proper precaution, but she now became aware of no-one else doing\n",
            "this and fled, leaving only her skirt to her right, which remained\n",
            "completely unalarmed.\n",
            "\n",
            "Hearing these words from the door made my sister realise that\n",
            "we had been carrying out some very serious business, one she could\n",
            "not deny. She stepped back out onto the landing, her legs spread\n",
            "peacefully between us.\n",
            "\n",
            "She was greatly encouraged by the way Grete put in her\n",
            "a very strong request for donations in return for which she\n",
            "coulding gladly given her place in the dinner group. But the\n",
            "other evening as we talked she became lethargic again, and seemed to\n",
            "want to cap here and there a question as to whether we should go to sleep\n",
            "and go to sleep immediately or sleep for a while longer. Then,\n",
            "overwhelmed with emotion, she suddenly jumped up and\n",
            "stretched her right hand out from under her skirt, and Grete followed\n",
            "that up with her right hand raised high in her smooth little\n",
            "ear as if she were a judge. She made a movement as\n",
            "she went, but the other women immediately followed it, some\n",
            "closer to home than others, and the women who had been in the\n",
            "posest way struck with it. Grete held her hand out\n",
            "to each and every one of them and smiled warmly. All three\n",
            "found this to be an enormous relief as it meant that no-one would\n",
            "have to live with this sad family for the rest of their lives.\n",
            "\n",
            "They certainly did.\n",
            "\n",
            "Tonight, Grete’s fourteenth birthday is near; her friends are\n",
            "now at work and she’s nearly ready to go for work. So she goes\n",
            "quietly round the room. One of the men—someone Grete had\n",
            "asked as a warning not to—calls out: “Come and celebrate his birthday\n",
            "here! It’s his birthday. Come and celebrate him here, for God’s sake!” And Grete\n",
            "does exactly as he says, albeit with a bit more politeness. And the\n",
            "others quickly look on in derision as well. But she nonetheless\n",
            "calls out: “Yes, Father? “Is it too late for that, really?\n",
            "You really need to remember that Grete’s birthday, though, isn’t it? And as she\n",
            "looks round the room she slowly withdraws her handbags as if she\n",
            "couldn’t believe what she had just seen. The furniture, of course, were\n",
            "much harder to see—the walls were too narrow for a train—and so Grete\n",
            "wouldn’t have been able to make out what she was\n",
            "looked at as the chief magistrate in the flat. Grete made a movement as\n",
            "she went\n",
            "“Close this door. We’ll go in there easy, before the door bursts open.\n",
            "No-one can enter the flat unless there’s a strong public order\n",
            "law law passed. As it is, then no-one can enter the flat.\n",
            "And who can enter the flat? There is no public order.\n",
            "No-one can enter the flat. Why? The laws are there to\n",
            "make sure nobody violates any of them. That’s why I’m locking\n",
            "the door. No-one wants to enter the flat. It’s enough to\n",
            "confirm that I have locked the door. “Listen, Grete”, says the\n",
            "\n",
            "[101 | 466.29] loss=0.69 avg=1.60\n",
            "[102 | 470.66] loss=0.59 avg=1.59\n",
            "[103 | 475.01] loss=0.51 avg=1.57\n",
            "[104 | 479.37] loss=0.59 avg=1.56\n",
            "[105 | 483.74] loss=0.44 avg=1.54\n",
            "[106 | 488.10] loss=0.39 avg=1.52\n",
            "[107 | 492.46] loss=0.56 avg=1.51\n",
            "[108 | 496.84] loss=0.44 avg=1.49\n",
            "[109 | 501.22] loss=0.43 avg=1.47\n",
            "[110 | 505.57] loss=0.49 avg=1.46\n",
            "[111 | 509.94] loss=0.46 avg=1.45\n",
            "[112 | 514.29] loss=0.35 avg=1.43\n",
            "[113 | 518.67] loss=0.40 avg=1.41\n",
            "[114 | 523.02] loss=0.21 avg=1.40\n",
            "[115 | 527.39] loss=0.37 avg=1.38\n",
            "[116 | 531.73] loss=0.40 avg=1.37\n",
            "[117 | 536.10] loss=0.19 avg=1.35\n",
            "[118 | 540.48] loss=0.33 avg=1.34\n",
            "[119 | 544.83] loss=0.23 avg=1.32\n",
            "[120 | 549.20] loss=0.41 avg=1.31\n",
            "[121 | 553.58] loss=0.23 avg=1.29\n",
            "[122 | 557.97] loss=0.25 avg=1.28\n",
            "[123 | 562.35] loss=0.30 avg=1.26\n",
            "[124 | 566.71] loss=0.27 avg=1.25\n",
            "[125 | 571.10] loss=0.26 avg=1.23\n",
            "[126 | 575.47] loss=0.22 avg=1.22\n",
            "[127 | 579.85] loss=0.21 avg=1.21\n",
            "[128 | 584.23] loss=0.23 avg=1.19\n",
            "[129 | 588.61] loss=0.33 avg=1.18\n",
            "[130 | 592.97] loss=0.23 avg=1.17\n",
            "[131 | 597.35] loss=0.21 avg=1.16\n",
            "[132 | 601.72] loss=0.26 avg=1.14\n",
            "[133 | 606.10] loss=0.12 avg=1.13\n",
            "[134 | 610.47] loss=0.20 avg=1.12\n",
            "[135 | 614.85] loss=0.14 avg=1.10\n",
            "[136 | 619.23] loss=0.25 avg=1.09\n",
            "[137 | 623.60] loss=0.18 avg=1.08\n",
            "[138 | 627.98] loss=0.25 avg=1.07\n",
            "[139 | 632.35] loss=0.13 avg=1.06\n",
            "[140 | 636.71] loss=0.22 avg=1.05\n",
            "[141 | 641.07] loss=0.12 avg=1.03\n",
            "[142 | 645.44] loss=0.11 avg=1.02\n",
            "[143 | 649.81] loss=0.14 avg=1.01\n",
            "[144 | 654.19] loss=0.14 avg=1.00\n",
            "[145 | 658.54] loss=0.34 avg=0.99\n",
            "[146 | 662.92] loss=0.14 avg=0.98\n",
            "[147 | 667.28] loss=0.19 avg=0.97\n",
            "[148 | 671.66] loss=0.17 avg=0.96\n",
            "[149 | 676.02] loss=0.19 avg=0.95\n",
            "[150 | 680.39] loss=0.11 avg=0.94\n",
            "[151 | 684.74] loss=0.09 avg=0.93\n",
            "[152 | 689.12] loss=0.16 avg=0.92\n",
            "[153 | 693.49] loss=0.08 avg=0.91\n",
            "[154 | 697.84] loss=0.11 avg=0.90\n",
            "[155 | 702.20] loss=0.11 avg=0.89\n",
            "[156 | 706.57] loss=0.09 avg=0.88\n",
            "[157 | 710.94] loss=0.11 avg=0.87\n",
            "[158 | 715.31] loss=0.09 avg=0.86\n",
            "[159 | 719.67] loss=0.11 avg=0.85\n",
            "[160 | 724.04] loss=0.09 avg=0.84\n",
            "[161 | 728.41] loss=0.10 avg=0.83\n",
            "[162 | 732.77] loss=0.12 avg=0.82\n",
            "[163 | 737.14] loss=0.11 avg=0.81\n",
            "[164 | 741.53] loss=0.09 avg=0.80\n",
            "[165 | 745.90] loss=0.08 avg=0.79\n",
            "[166 | 750.29] loss=0.11 avg=0.78\n",
            "[167 | 754.65] loss=0.08 avg=0.78\n",
            "[168 | 759.02] loss=0.09 avg=0.77\n",
            "[169 | 763.39] loss=0.07 avg=0.76\n",
            "[170 | 767.76] loss=0.08 avg=0.75\n",
            "[171 | 772.12] loss=0.08 avg=0.74\n",
            "[172 | 776.48] loss=0.07 avg=0.73\n",
            "[173 | 780.86] loss=0.07 avg=0.73\n",
            "[174 | 785.22] loss=0.07 avg=0.72\n",
            "[175 | 789.58] loss=0.06 avg=0.71\n",
            "[176 | 793.93] loss=0.09 avg=0.70\n",
            "[177 | 798.29] loss=0.09 avg=0.70\n",
            "[178 | 802.65] loss=0.06 avg=0.69\n",
            "[179 | 807.01] loss=0.08 avg=0.68\n",
            "[180 | 811.37] loss=0.09 avg=0.67\n",
            "[181 | 815.73] loss=0.06 avg=0.67\n",
            "[182 | 820.10] loss=0.08 avg=0.66\n",
            "[183 | 824.45] loss=0.06 avg=0.65\n",
            "[184 | 828.83] loss=0.07 avg=0.65\n",
            "[185 | 833.19] loss=0.07 avg=0.64\n",
            "[186 | 837.55] loss=0.06 avg=0.63\n",
            "[187 | 841.91] loss=0.06 avg=0.62\n",
            "[188 | 846.27] loss=0.05 avg=0.62\n",
            "[189 | 850.65] loss=0.06 avg=0.61\n",
            "[190 | 855.02] loss=0.10 avg=0.61\n",
            "[191 | 859.39] loss=0.06 avg=0.60\n",
            "[192 | 863.76] loss=0.08 avg=0.59\n",
            "[193 | 868.13] loss=0.08 avg=0.59\n",
            "[194 | 872.51] loss=0.07 avg=0.58\n",
            "[195 | 876.87] loss=0.08 avg=0.57\n",
            "[196 | 881.24] loss=0.07 avg=0.57\n",
            "[197 | 885.61] loss=0.05 avg=0.56\n",
            "[198 | 889.98] loss=0.04 avg=0.56\n",
            "[199 | 894.34] loss=0.06 avg=0.55\n",
            "[200 | 898.71] loss=0.06 avg=0.55\n",
            "======== SAMPLE 1 ========\n",
            ".\n",
            "\n",
            "He was already speaking so slowly and with such an exaggerated voice, but now\n",
            "of course his father no longer held back the words with his hand as he\n",
            "carries them out again and again. “This time, then,\n",
            "he’s more than willing to sacrifice some of his privacy for his own\n",
            "pity; he’s already used up all the time he’s paid into his own pocket, and will\n",
            "even have to take out a large part of it if he wants to show his gratitude\n",
            "at once, as he does not have any other choice than to take\n",
            "his father’s money and carry on with his day-to-day life.\n",
            "\n",
            "But being able to speak this way should not be taken as proof that you are\n",
            "gentlemanly, as such speaking probably would not be appreciated\n",
            "by everyone, but rather as a sign that you might not be\n",
            "inordinately observant, and as such you’ll should endeavor\n",
            "to establish a friendly distance from the people you’re speaking to\n",
            "and do so in a manner that is both friendly and considerate. Be\n",
            " observant,, however, and do not abandon the obvious object—a\n",
            "carpeted area along the bank of the Amiens—which shows that you\n",
            "would like to take notice, and pay particular attention to the large amount of\n",
            "paper and cloth which covers the floor and ceiling, as well as the wooden\n",
            "blocks placed therefrom. For the time being, of course, you’ll be\n",
            "allowed to sketch the whole of this flat out and should be encouraged\n",
            "by the fact that it contains a library full of ancient\n",
            "texts—particularly the works of Zen master Gautama—which have been painstakingly\n",
            "copied and republished hundreds of times, each time with its own set of\n",
            "piercing restrictions. Be sure to bear in mind that these restrictions never\n",
            "end or even come into any way—unless you have much reason to mistrust such\n",
            "proper direction as Gautama has shown us. Anyway, enough\n",
            "praising!—but there was more to it. During the whole of\n",
            "the evening, the two men sat cross-legged at the\n",
            "table, both of them exhausted from working so hard, and there were\n",
            "parties of both sexes who were there to listen to what was being\n",
            "said. One of the gentlemen, a tall and lean individual with short\n",
            "brown hair that flapped around his head, was asked to give his\n",
            "displayed address. The two gentlemen in the middle had turned\n",
            "round quite easily, and Gregor remained seated at his table.\n",
            "The chief clerk, however, had left the matter for him. The chief\n",
            "clerk, since dismissed from his post, had now appeared before the\n",
            "convenor and stood before the chief clerk, who had now taken his\n",
            "apparent place, clearly wishing him well and requesting that he\n",
            "report his behaviour as quickly as possible. The three gentlemen had already\n",
            "persuaded him that they believed him of his doing this, and he\n",
            "hashed them all out. The three gentlemen now took their seats in the\n",
            "convenor’s table. The chief clerk stood, facing the door and\n",
            "smoking his joint, and read two or three lines of the chief abbreviation for\n",
            "the chief. The three gentlemen sat cross leant in the main room. The\n",
            "other two took their seats in the lower room, and stared out of the\n",
            "convenor’s lamp. They had never heard the chief before, and it was almost\n",
            "certain that he had not heard them again, that was all he could say. “Seven o’clock, anyway,”,\n",
            "said the chief clerk, now speaking to the chief clerk in the same room\n",
            "with a kind of pride that made him feel small, and without “a\n",
            "word of protest”. The chief clerk had been standing there looking\n",
            "pale and effortless for hours now, and had not a single sound but the sound of\n",
            "the chief’s spit hitting the floor came echoing out across the room. “My\n",
            "sister”, the chief said, “did you know that when you came in here that\n",
            "I’m greatly encouraged, and that nothing dings?” That was the first word\n",
            "sent his way. The way he spoke, in short, confirmed what everyone was saying—that\n",
            "Samsa’mir Rahman was an ancient and fearsome being, and a threat to the\n",
            "people of the flat’s at present. The way he treated his tenants was\n",
            "highly questionable, for instance, he might have knocked on the door of a\n",
            "housekeeper’s home during the night and found no response, and even\n",
            "somewhat annoyed, he called into the power of his bedroom, but he had neglected to\n",
            "start again with the previous night�\n",
            "\n",
            "[201 | 919.57] loss=0.06 avg=0.54\n",
            "[202 | 923.93] loss=0.06 avg=0.53\n",
            "[203 | 928.28] loss=0.06 avg=0.53\n",
            "[204 | 932.64] loss=0.05 avg=0.52\n",
            "[205 | 937.00] loss=0.06 avg=0.52\n",
            "[206 | 941.37] loss=0.06 avg=0.51\n",
            "[207 | 945.72] loss=0.05 avg=0.51\n",
            "[208 | 950.07] loss=0.06 avg=0.50\n",
            "[209 | 954.43] loss=0.06 avg=0.50\n",
            "[210 | 958.79] loss=0.06 avg=0.49\n",
            "[211 | 963.15] loss=0.06 avg=0.49\n",
            "[212 | 967.52] loss=0.06 avg=0.48\n",
            "[213 | 971.89] loss=0.05 avg=0.48\n",
            "[214 | 976.26] loss=0.05 avg=0.47\n",
            "[215 | 980.62] loss=0.05 avg=0.47\n",
            "[216 | 984.99] loss=0.06 avg=0.46\n",
            "[217 | 989.35] loss=0.06 avg=0.46\n",
            "[218 | 993.74] loss=0.07 avg=0.45\n",
            "[219 | 998.11] loss=0.07 avg=0.45\n",
            "[220 | 1002.46] loss=0.05 avg=0.45\n",
            "[221 | 1006.82] loss=0.06 avg=0.44\n",
            "[222 | 1011.19] loss=0.05 avg=0.44\n",
            "[223 | 1015.57] loss=0.05 avg=0.43\n",
            "[224 | 1019.96] loss=0.05 avg=0.43\n",
            "[225 | 1024.33] loss=0.06 avg=0.42\n",
            "[226 | 1028.70] loss=0.08 avg=0.42\n",
            "[227 | 1033.05] loss=0.04 avg=0.42\n",
            "[228 | 1037.41] loss=0.05 avg=0.41\n",
            "[229 | 1041.78] loss=0.05 avg=0.41\n",
            "[230 | 1046.14] loss=0.07 avg=0.40\n",
            "[231 | 1050.52] loss=0.06 avg=0.40\n",
            "[232 | 1054.89] loss=0.05 avg=0.40\n",
            "[233 | 1059.25] loss=0.06 avg=0.39\n",
            "[234 | 1063.63] loss=0.06 avg=0.39\n",
            "[235 | 1067.99] loss=0.07 avg=0.39\n",
            "[236 | 1072.37] loss=0.06 avg=0.38\n",
            "[237 | 1076.75] loss=0.04 avg=0.38\n",
            "[238 | 1081.13] loss=0.05 avg=0.37\n",
            "[239 | 1085.51] loss=0.04 avg=0.37\n",
            "[240 | 1089.89] loss=0.05 avg=0.37\n",
            "[241 | 1094.27] loss=0.06 avg=0.36\n",
            "[242 | 1098.64] loss=0.04 avg=0.36\n",
            "[243 | 1103.03] loss=0.05 avg=0.36\n",
            "[244 | 1107.41] loss=0.05 avg=0.35\n",
            "[245 | 1111.79] loss=0.06 avg=0.35\n",
            "[246 | 1116.17] loss=0.05 avg=0.35\n",
            "[247 | 1120.53] loss=0.04 avg=0.34\n",
            "[248 | 1124.92] loss=0.05 avg=0.34\n",
            "[249 | 1129.29] loss=0.05 avg=0.34\n",
            "[250 | 1133.68] loss=0.04 avg=0.33\n",
            "[251 | 1138.06] loss=0.06 avg=0.33\n",
            "[252 | 1142.44] loss=0.05 avg=0.33\n",
            "[253 | 1146.80] loss=0.05 avg=0.33\n",
            "[254 | 1151.17] loss=0.05 avg=0.32\n",
            "[255 | 1155.54] loss=0.05 avg=0.32\n",
            "[256 | 1159.91] loss=0.06 avg=0.32\n",
            "[257 | 1164.28] loss=0.05 avg=0.31\n",
            "[258 | 1168.65] loss=0.05 avg=0.31\n",
            "[259 | 1173.03] loss=0.04 avg=0.31\n",
            "[260 | 1177.38] loss=0.05 avg=0.31\n",
            "[261 | 1181.75] loss=0.06 avg=0.30\n",
            "[262 | 1186.12] loss=0.06 avg=0.30\n",
            "[263 | 1190.49] loss=0.05 avg=0.30\n",
            "[264 | 1194.84] loss=0.06 avg=0.29\n",
            "[265 | 1199.18] loss=0.04 avg=0.29\n",
            "[266 | 1203.53] loss=0.05 avg=0.29\n",
            "[267 | 1207.88] loss=0.04 avg=0.29\n",
            "[268 | 1212.24] loss=0.06 avg=0.28\n",
            "[269 | 1216.59] loss=0.05 avg=0.28\n",
            "[270 | 1220.94] loss=0.05 avg=0.28\n",
            "[271 | 1225.30] loss=0.05 avg=0.28\n",
            "[272 | 1229.65] loss=0.05 avg=0.27\n",
            "[273 | 1233.99] loss=0.04 avg=0.27\n",
            "[274 | 1238.34] loss=0.05 avg=0.27\n",
            "[275 | 1242.70] loss=0.05 avg=0.27\n",
            "[276 | 1247.05] loss=0.05 avg=0.26\n",
            "[277 | 1251.41] loss=0.05 avg=0.26\n",
            "[278 | 1255.78] loss=0.05 avg=0.26\n",
            "[279 | 1260.14] loss=0.05 avg=0.26\n",
            "[280 | 1264.51] loss=0.04 avg=0.26\n",
            "[281 | 1268.88] loss=0.06 avg=0.25\n",
            "[282 | 1273.25] loss=0.05 avg=0.25\n",
            "[283 | 1277.61] loss=0.05 avg=0.25\n",
            "[284 | 1281.99] loss=0.06 avg=0.25\n",
            "[285 | 1286.36] loss=0.04 avg=0.25\n",
            "[286 | 1290.75] loss=0.05 avg=0.24\n",
            "[287 | 1295.12] loss=0.05 avg=0.24\n",
            "[288 | 1299.49] loss=0.05 avg=0.24\n",
            "[289 | 1303.87] loss=0.03 avg=0.24\n",
            "[290 | 1308.25] loss=0.05 avg=0.23\n",
            "[291 | 1312.62] loss=0.05 avg=0.23\n",
            "[292 | 1316.98] loss=0.06 avg=0.23\n",
            "[293 | 1321.34] loss=0.03 avg=0.23\n",
            "[294 | 1325.69] loss=0.03 avg=0.23\n",
            "[295 | 1330.07] loss=0.04 avg=0.22\n",
            "[296 | 1334.44] loss=0.04 avg=0.22\n",
            "[297 | 1338.81] loss=0.05 avg=0.22\n",
            "[298 | 1343.19] loss=0.04 avg=0.22\n",
            "[299 | 1347.56] loss=0.04 avg=0.22\n",
            "[300 | 1351.93] loss=0.05 avg=0.22\n",
            "======== SAMPLE 1 ========\n",
            " behind, but they quickly became familiar with it, as\n",
            "they learned from the numerous family members along the way that Gregor\n",
            "took everything in at once, left the worksweers completely clean,\n",
            "and, most of all, did not she appear to anyone on the charwoman\n",
            "impatiently? It was impossible for them to live with this state,\n",
            "especially as work was so difficult; but if Gregor had only been able to\n",
            "sit up at the windowfix and listen to the dull music instead of\n",
            "carpeting and wasting his time wandering about uselessly?\n",
            "But no-one would have thought of that. Come the autumn, the months;\n",
            "the barren, grey hills and the empty streets of Prague—all the time\n",
            "conveniently left up even by chance—wouldGregor’s memory be\n",
            "reappeared in some way, picture in his head as well as something beautiful.\n",
            "His sister would often spend the whole night in the same room with him,\n",
            "though Gregor needed a good strong bed to keep him from turning into the\n",
            "next room and, as a precaution, kept a slight window open each evening when\n",
            "he was tired. And Grete would often spend the whole night in the same room with\n",
            "him, closed the door frequently to see what she would say when she awoke.\n",
            "But Grete had become much too used to it, and had lost interest in\n",
            "herpetological matters. Now, of course, she could not refuse the compliments,\n",
            "so they could not be short of money. Grete would often send an illustrated\n",
            "card with the full name of the guest and, if she were asked to add a few\n",
            "words of his own, she would be put in the directorship of the family.\n",
            "Her parents, of course, could not bear to see Gregor taken away, and\n",
            "they would go to great expense to persuade him to come in, show them his\n",
            "way, pay their respects to him on the way, and accept the fact that he\n",
            "had no particular use for the whole of Prague.\n",
            "\n",
            "So he refused to take the proposal, and spent the evening in\n",
            "the same room with his room radio tuned to hisriad\n",
            "calls, listening to which he would have none of it out when he awoke.\n",
            "He was soon joined, one by one, by the short person, the tall\n",
            "and lanky, Grete. She would go into the other room pitched\n",
            "battlely with the short person, Grete who, for her part, was robust\n",
            "and thought about her ways without moving so much as flap her arms about\n",
            "attractedly. And, since they knew each other personally, who would go so\n",
            "close to the chief clerk if she did not have to—Grete had a very good\n",
            "friend whom she knew very well and whom Grete had sent to several\n",
            "cities in her plan. They would laugh uproariously about the poor\n",
            "woman, the beggar, the butcher, the vagabond, the stranger; about the\n",
            "violin, the one who divided the time between them during their stay.\n",
            "There was much whispering and much whispering in the adjoining room, which\n",
            "was surrounded mostly by the familiar smell of leather after\n",
            "furniture had been taken away. The chief clerk, in his\n",
            "uniform, and the maid were silent. The maid had a kind of\n",
            "clutch, but it was not very pleasant. The three gentlemen stepped out\n",
            "from under the couch and looked round in astonishment. The chief clerk was\n",
            "shaking heavily, Mr. Samsa was raising his voice, and the three gentlemen\n",
            "clenched their fists as if wanting to show their approval. Now they\n",
            "pressed their heads into the void and looked round in disappointment.\n",
            "The door was slammed shut; the chief clerk, stepping out\n",
            "from under the couch, stood shaking with the others. The chief\n",
            "clerk, looking exhausted from laying waste to begin with, squealed in\n",
            "his chair as if he could do with it completely. The chief clerk,\n",
            "clenching tightly in his chair, began to move towards the chief clerk.\n",
            "The three gentlemen drew their beards and tried to grasp at the chief\n",
            "clerk’s hold. The chief clerk seemed to have realised the chief’s\n",
            "attribution and looked into his eyes, but his expression remained unchanged.\n",
            "Because of his agitation, the chief clerk had to withdraw the chair he\n",
            "had been holding himself and go back to his desk. As he went he became more and\n",
            "more frequent and frequentable in his clutches it became very obvious to him that no\n",
            "therable decision had been taken and he became quite resentful of the attention\n",
            "himself was paying him. He became so occupied in pushing himself against the\n",
            "corridor that he forgot all the directions and had to rely on his\n",
            "keys to navigate himself. He gave chase after the chief clerk, who was slowly\n",
            "let down in\n",
            "\n",
            "[301 | 1372.76] loss=0.04 avg=0.21\n",
            "[302 | 1377.14] loss=0.04 avg=0.21\n",
            "[303 | 1381.50] loss=0.04 avg=0.21\n",
            "[304 | 1385.87] loss=0.03 avg=0.21\n",
            "[305 | 1390.23] loss=0.04 avg=0.21\n",
            "[306 | 1394.58] loss=0.06 avg=0.20\n",
            "[307 | 1398.94] loss=0.04 avg=0.20\n",
            "[308 | 1403.29] loss=0.04 avg=0.20\n",
            "[309 | 1407.65] loss=0.05 avg=0.20\n",
            "[310 | 1412.00] loss=0.06 avg=0.20\n",
            "[311 | 1416.35] loss=0.03 avg=0.20\n",
            "[312 | 1420.71] loss=0.04 avg=0.20\n",
            "[313 | 1425.07] loss=0.04 avg=0.19\n",
            "[314 | 1429.42] loss=0.05 avg=0.19\n",
            "[315 | 1433.77] loss=0.04 avg=0.19\n",
            "[316 | 1438.13] loss=0.05 avg=0.19\n",
            "[317 | 1442.48] loss=0.06 avg=0.19\n",
            "[318 | 1446.83] loss=0.11 avg=0.19\n",
            "[319 | 1451.18] loss=0.04 avg=0.19\n",
            "[320 | 1455.54] loss=0.04 avg=0.18\n",
            "[321 | 1459.91] loss=0.04 avg=0.18\n",
            "[322 | 1464.27] loss=0.04 avg=0.18\n",
            "[323 | 1468.63] loss=0.05 avg=0.18\n",
            "[324 | 1472.98] loss=0.06 avg=0.18\n",
            "[325 | 1477.33] loss=0.05 avg=0.18\n",
            "[326 | 1481.69] loss=0.04 avg=0.18\n",
            "[327 | 1486.06] loss=0.05 avg=0.17\n",
            "[328 | 1490.43] loss=0.05 avg=0.17\n",
            "[329 | 1494.82] loss=0.04 avg=0.17\n",
            "[330 | 1499.18] loss=0.06 avg=0.17\n",
            "[331 | 1503.56] loss=0.04 avg=0.17\n",
            "[332 | 1507.94] loss=0.05 avg=0.17\n",
            "[333 | 1512.30] loss=0.04 avg=0.17\n",
            "[334 | 1516.68] loss=0.06 avg=0.17\n",
            "[335 | 1521.06] loss=0.04 avg=0.16\n",
            "[336 | 1525.43] loss=0.04 avg=0.16\n",
            "[337 | 1529.80] loss=0.06 avg=0.16\n",
            "[338 | 1534.16] loss=0.04 avg=0.16\n",
            "[339 | 1538.54] loss=0.04 avg=0.16\n",
            "[340 | 1542.91] loss=0.04 avg=0.16\n",
            "[341 | 1547.29] loss=0.03 avg=0.16\n",
            "[342 | 1551.68] loss=0.06 avg=0.16\n",
            "[343 | 1556.05] loss=0.04 avg=0.15\n",
            "[344 | 1560.42] loss=0.05 avg=0.15\n",
            "[345 | 1564.79] loss=0.03 avg=0.15\n",
            "[346 | 1569.17] loss=0.04 avg=0.15\n",
            "[347 | 1573.53] loss=0.03 avg=0.15\n",
            "[348 | 1577.91] loss=0.05 avg=0.15\n",
            "[349 | 1582.28] loss=0.05 avg=0.15\n",
            "[350 | 1586.65] loss=0.04 avg=0.15\n",
            "[351 | 1591.02] loss=0.05 avg=0.15\n",
            "[352 | 1595.38] loss=0.04 avg=0.14\n",
            "[353 | 1599.74] loss=0.05 avg=0.14\n",
            "[354 | 1604.12] loss=0.05 avg=0.14\n",
            "[355 | 1608.47] loss=0.04 avg=0.14\n",
            "[356 | 1612.84] loss=0.05 avg=0.14\n",
            "[357 | 1617.21] loss=0.04 avg=0.14\n",
            "[358 | 1621.59] loss=0.05 avg=0.14\n",
            "[359 | 1625.96] loss=0.04 avg=0.14\n",
            "[360 | 1630.32] loss=0.04 avg=0.14\n",
            "[361 | 1634.71] loss=0.03 avg=0.14\n",
            "[362 | 1639.07] loss=0.05 avg=0.13\n",
            "[363 | 1643.45] loss=0.04 avg=0.13\n",
            "[364 | 1647.83] loss=0.05 avg=0.13\n",
            "[365 | 1652.20] loss=0.05 avg=0.13\n",
            "[366 | 1656.57] loss=0.04 avg=0.13\n",
            "[367 | 1660.94] loss=0.05 avg=0.13\n",
            "[368 | 1665.31] loss=0.04 avg=0.13\n",
            "[369 | 1669.68] loss=0.04 avg=0.13\n",
            "[370 | 1674.06] loss=0.05 avg=0.13\n",
            "[371 | 1678.43] loss=0.05 avg=0.13\n",
            "[372 | 1682.80] loss=0.04 avg=0.13\n",
            "[373 | 1687.17] loss=0.04 avg=0.12\n",
            "[374 | 1691.54] loss=0.05 avg=0.12\n",
            "[375 | 1695.91] loss=0.05 avg=0.12\n",
            "[376 | 1700.28] loss=0.04 avg=0.12\n",
            "[377 | 1704.65] loss=0.04 avg=0.12\n",
            "[378 | 1709.03] loss=0.04 avg=0.12\n",
            "[379 | 1713.41] loss=0.05 avg=0.12\n",
            "[380 | 1717.78] loss=0.04 avg=0.12\n",
            "[381 | 1722.15] loss=0.04 avg=0.12\n",
            "[382 | 1726.51] loss=0.03 avg=0.12\n",
            "[383 | 1730.87] loss=0.03 avg=0.12\n",
            "[384 | 1735.24] loss=0.05 avg=0.12\n",
            "[385 | 1739.61] loss=0.05 avg=0.11\n",
            "[386 | 1743.98] loss=0.05 avg=0.11\n",
            "[387 | 1748.34] loss=0.04 avg=0.11\n",
            "[388 | 1752.72] loss=0.04 avg=0.11\n",
            "[389 | 1757.09] loss=0.03 avg=0.11\n",
            "[390 | 1761.47] loss=0.06 avg=0.11\n",
            "[391 | 1765.84] loss=0.06 avg=0.11\n",
            "[392 | 1770.21] loss=0.04 avg=0.11\n",
            "[393 | 1774.58] loss=0.04 avg=0.11\n",
            "[394 | 1778.95] loss=0.04 avg=0.11\n",
            "[395 | 1783.32] loss=0.04 avg=0.11\n",
            "[396 | 1787.71] loss=0.04 avg=0.11\n",
            "[397 | 1792.09] loss=0.05 avg=0.11\n",
            "[398 | 1796.48] loss=0.05 avg=0.11\n",
            "[399 | 1800.84] loss=0.03 avg=0.11\n",
            "[400 | 1805.20] loss=0.04 avg=0.10\n",
            "======== SAMPLE 1 ========\n",
            " not any need for it. No-one had asked for it, and though Gregor had most certainly never asked\n",
            "for it, he could think of no other way of explaining it. And although\n",
            "he certainly did not need to hear the chief clerk’s explanation,\n",
            "he did not sit still for long at the office, look back over his shoulder\n",
            "at the chair in his father’s arms, and feel his way with all his strength\n",
            "against the weight of his work. He would have wanted to open the door, but his\n",
            "father’s presence was there too much for him to handle and he would have\n",
            "let his boss bring his mother to save him the trouble.” Gregor’s words had been\n",
            "better describing things, but his mother was another matter. He had to admit that\n",
            "he had pushed his head down several steps too far, and that really\n",
            "put him in the embarrassing position of locking the door when, before, they\n",
            "had been in the same place, but now it was much harder. He gave in his\n",
            "toominies and went back for his mother; her hands and his sister’s\n",
            "whole turned away when they saw his helplessness; his mother looked on\n",
            "as if she had never beenheld, and stamped her foot. Gregor’s sister, however,\n",
            "found the impression too: she turned her eye to the sun\n",
            "and saw it perfect; her face was PUSS! That made her laugh;\n",
            "her hands were covered in those little legs that grew out from under\n",
            "the couch to hold Gregor tightly; her laugh was full of promise;\n",
            "“And we’ll make you some good food too</��\n",
            "“And for just one linole day you'll be able to eat at home too</��\n",
            "So Gregor and his sister set off without delay, laden with\n",
            "milk, which they had consumed in this world for countless times their\n",
            "diet was inadequate. The journey was short, but indeed they\n",
            "went at it with intent. They decided the best way to make use of\n",
            "that day was for relaxation. They had reached the station and were\n",
            "clenching their fists in the air while theyolly. Melody began to\n",
            "sleep. She would normally do so this morning, as well as as every evening\n",
            "afternoon as well as morning as she went to bed, but all she could think of was\n",
            "cooking and cleaning. A few hours later she woke warm and\n",
            "sensible from her slumber. She could already smell the fresh air from the\n",
            "carpet. It was a pleasant surprise, considering that no-one had\n",
            "suddenly taken him by the hand and been injured, but it still\n",
            "must have been bad news. It was night by night. “The clouds have stopped\n",
            "crawling, then’sad day, they’re still there and we’re just a\n",
            "companion for their father and mother”, said his father as if he could\n",
            "see the fragility of his son’s memory, and backed away from the door as\n",
            "him sister did the same. The door was slammed shut again from the front.\n",
            "Gregor’s sister began to cry. He was so engrossed in his work that\n",
            "he almost forgot that his mother was there, or at least was looking\n",
            "at it with disgust. He began to play the card: “Against All Authority, by Grete\n",
            "Gregor, 2016. Thanks, Mrs. Gregor.” “I’ll send you an e-mail whenever I’m\n",
            "able to, just so you know, tell you what’s been happen.” And he read it over\n",
            "again, each time with a warning deep in his voice: “Bye my lady! Don’t tow my\n",
            "trouser!” “I’ll warn you twice now. First, about how I’m\n",
            "unable to drive, and secondly, that everything’s been going so slowly.\n",
            "I’m getting up now.” And he sounded as if he were alarmed as he let go\n",
            "of the door, which was slammed shut again with the stick it was part of his\n",
            "family provided he could speak when his parents were not around. “What’s been\n",
            "happening?”, asked his father, who was still in the same state as\n",
            "he had been in before, and added “I’ll be there in a moment.”\n",
            "\n",
            "The door was slammed shut again with the stick used to lock it in the\n",
            "kitchen. Now it was Gregor who came into the room, broke into a\n",
            "friendly laugh, and stretched out his hand.\n",
            "\n",
            "Gregor hardly had a month to live on before his mother died, a loss to him\n",
            "all the time\n",
            "\n",
            "[401 | 1826.22] loss=0.04 avg=0.10\n",
            "[402 | 1830.59] loss=0.04 avg=0.10\n",
            "[403 | 1834.95] loss=0.03 avg=0.10\n",
            "[404 | 1839.32] loss=0.03 avg=0.10\n",
            "[405 | 1843.69] loss=0.04 avg=0.10\n",
            "[406 | 1848.05] loss=0.05 avg=0.10\n",
            "[407 | 1852.42] loss=0.04 avg=0.10\n",
            "[408 | 1856.80] loss=0.04 avg=0.10\n",
            "[409 | 1861.15] loss=0.04 avg=0.10\n",
            "[410 | 1865.53] loss=0.04 avg=0.10\n",
            "[411 | 1869.90] loss=0.04 avg=0.10\n",
            "[412 | 1874.27] loss=0.03 avg=0.10\n",
            "[413 | 1878.64] loss=0.04 avg=0.10\n",
            "[414 | 1883.01] loss=0.03 avg=0.10\n",
            "[415 | 1887.39] loss=0.04 avg=0.10\n",
            "[416 | 1891.75] loss=0.05 avg=0.09\n",
            "[417 | 1896.13] loss=0.04 avg=0.09\n",
            "[418 | 1900.50] loss=0.04 avg=0.09\n",
            "[419 | 1904.88] loss=0.04 avg=0.09\n",
            "[420 | 1909.24] loss=0.04 avg=0.09\n",
            "[421 | 1913.61] loss=0.03 avg=0.09\n",
            "[422 | 1917.99] loss=0.05 avg=0.09\n",
            "[423 | 1922.34] loss=0.03 avg=0.09\n",
            "[424 | 1926.72] loss=0.04 avg=0.09\n",
            "[425 | 1931.08] loss=0.03 avg=0.09\n",
            "[426 | 1935.44] loss=0.03 avg=0.09\n",
            "[427 | 1939.80] loss=0.05 avg=0.09\n",
            "[428 | 1944.16] loss=0.03 avg=0.09\n",
            "[429 | 1948.53] loss=0.04 avg=0.09\n",
            "[430 | 1952.87] loss=0.05 avg=0.09\n",
            "[431 | 1957.22] loss=0.04 avg=0.09\n",
            "[432 | 1961.59] loss=0.04 avg=0.09\n",
            "[433 | 1965.93] loss=0.03 avg=0.09\n",
            "[434 | 1970.30] loss=0.04 avg=0.09\n",
            "[435 | 1974.66] loss=0.04 avg=0.09\n",
            "[436 | 1979.03] loss=0.03 avg=0.08\n",
            "[437 | 1983.39] loss=0.04 avg=0.08\n",
            "[438 | 1987.76] loss=0.03 avg=0.08\n",
            "[439 | 1992.12] loss=0.03 avg=0.08\n",
            "[440 | 1996.50] loss=0.04 avg=0.08\n",
            "[441 | 2000.86] loss=0.04 avg=0.08\n",
            "[442 | 2005.23] loss=0.03 avg=0.08\n",
            "[443 | 2009.58] loss=0.05 avg=0.08\n",
            "[444 | 2013.94] loss=0.03 avg=0.08\n",
            "[445 | 2018.31] loss=0.06 avg=0.08\n",
            "[446 | 2022.68] loss=0.04 avg=0.08\n",
            "[447 | 2027.05] loss=0.03 avg=0.08\n",
            "[448 | 2031.43] loss=0.04 avg=0.08\n",
            "[449 | 2035.82] loss=0.04 avg=0.08\n",
            "[450 | 2040.18] loss=0.03 avg=0.08\n",
            "[451 | 2044.54] loss=0.04 avg=0.08\n",
            "[452 | 2048.91] loss=0.03 avg=0.08\n",
            "[453 | 2053.28] loss=0.04 avg=0.08\n",
            "[454 | 2057.64] loss=0.03 avg=0.08\n",
            "[455 | 2062.01] loss=0.03 avg=0.08\n",
            "[456 | 2066.38] loss=0.04 avg=0.08\n",
            "[457 | 2070.75] loss=0.03 avg=0.08\n",
            "[458 | 2075.11] loss=0.04 avg=0.07\n",
            "[459 | 2079.47] loss=0.04 avg=0.07\n",
            "[460 | 2083.82] loss=0.03 avg=0.07\n",
            "[461 | 2088.18] loss=0.04 avg=0.07\n",
            "[462 | 2092.53] loss=0.04 avg=0.07\n",
            "[463 | 2096.90] loss=0.04 avg=0.07\n",
            "[464 | 2101.26] loss=0.03 avg=0.07\n",
            "[465 | 2105.62] loss=0.04 avg=0.07\n",
            "[466 | 2109.98] loss=0.03 avg=0.07\n",
            "[467 | 2114.35] loss=0.04 avg=0.07\n",
            "[468 | 2118.71] loss=0.04 avg=0.07\n",
            "[469 | 2123.08] loss=0.04 avg=0.07\n",
            "[470 | 2127.46] loss=0.04 avg=0.07\n",
            "[471 | 2131.83] loss=0.04 avg=0.07\n",
            "[472 | 2136.20] loss=0.03 avg=0.07\n",
            "[473 | 2140.58] loss=0.03 avg=0.07\n",
            "[474 | 2144.95] loss=0.04 avg=0.07\n",
            "[475 | 2149.34] loss=0.04 avg=0.07\n",
            "[476 | 2153.70] loss=0.04 avg=0.07\n",
            "[477 | 2158.08] loss=0.03 avg=0.07\n",
            "[478 | 2162.46] loss=0.04 avg=0.07\n",
            "[479 | 2166.84] loss=0.03 avg=0.07\n",
            "[480 | 2171.22] loss=0.03 avg=0.07\n",
            "[481 | 2175.60] loss=0.03 avg=0.07\n",
            "[482 | 2179.98] loss=0.04 avg=0.07\n",
            "[483 | 2184.35] loss=0.03 avg=0.07\n",
            "[484 | 2188.73] loss=0.03 avg=0.07\n",
            "[485 | 2193.11] loss=0.04 avg=0.07\n",
            "[486 | 2197.48] loss=0.03 avg=0.07\n",
            "[487 | 2201.84] loss=0.04 avg=0.07\n",
            "[488 | 2206.22] loss=0.03 avg=0.06\n",
            "[489 | 2210.60] loss=0.03 avg=0.06\n",
            "[490 | 2214.98] loss=0.03 avg=0.06\n",
            "[491 | 2219.35] loss=0.03 avg=0.06\n",
            "[492 | 2223.72] loss=0.05 avg=0.06\n",
            "[493 | 2228.10] loss=0.04 avg=0.06\n",
            "[494 | 2232.47] loss=0.04 avg=0.06\n",
            "[495 | 2236.83] loss=0.03 avg=0.06\n",
            "[496 | 2241.19] loss=0.03 avg=0.06\n",
            "[497 | 2245.56] loss=0.04 avg=0.06\n",
            "[498 | 2249.92] loss=0.03 avg=0.06\n",
            "[499 | 2254.28] loss=0.04 avg=0.06\n",
            "[500 | 2258.63] loss=0.04 avg=0.06\n",
            "Saving checkpoint/run1/model-500\n",
            "s\n",
            "design\n",
            "\n",
            "did not leave the room. Nor would he ever have allowed himself the chance to\n",
            "see his sister, all the while his mother was ill and the family was in danger\n",
            "of losing the peace he had lost by living in this flat for so long. And despite all\n",
            "his sorrow, he still had not come up to his room to take his breakfast. As\n",
            "he lay there immobile and immobile, all around him the white noise as he\n",
            "broke and fell produced this image:\n",
            "his mother, his sister, his father, his mother in the next room followed\n",
            "by his father, who was not far away and much younger than his\n",
            "mother, and fell onto the floor; his shouts resounding all around him.\n",
            "\n",
            "“Leave my room, now!”, said his father almost as a question; than\n",
            "did he seem to have got over the shock of his discovery that his sister was\n",
            " well and, with them both working so hard, she was actually creating the\n",
            "chaos that now rocked the flat. It was not until late at night that the gaslight in\n",
            "this room was shut out, and then only when the three gentlemen had\n",
            "come round in their Rolls-Royce to take a lookat of the mess.\n",
            "They had decided that a hotel room on the lower floor, which\n",
            "was already used to standing room only, was more than enough room for one\n",
            "of the three gentlemen. The room on the right was for one servant, but the\n",
            "middle one, who was smaller, wanted to take us somewhere nice for\n",
            "that purpose. We were all set to go and get him soon enough, but he\n",
            "came hurrying off after him, and as he bent down the couch he hung\n",
            "fancy jewelry from it—jewellery from Europe, probably—that he could place\n",
            "near his lover when he was not at work. He also wanted to bring my mother and\n",
            "my father somewhere nice for their own use. So, without further ado, here they are,\n",
            "in their present state:\n",
            "Ronan’s Room\n",
            "\n",
            "This was a simple room with a few rooms in it arranged\n",
            "in such a way that they could all sleep in it. All the while,\n",
            "there was the sound of the door being pushed in and out, and that even\n",
            "when they were squarely in the doorway they would sometimes, once\n",
            "an hour, half-heartedly accepting the challenge and the invitation\n",
            "without even knowing what had been written in the front door about it. It\n",
            "was a regular room with seldom-ending arguments, and even if\n",
            "something went badly they would go back and try to get the better of it again. Even\n",
            "one for the season, which was an office year, this was no time to waste.\n",
            "The three gentlemen had already finished their meal, the one with the\n",
            "crying face had moved out of his room and the other two had become quiet.\n",
            "The gentleman in the middle, who had been countingenance, had become\n",
            "more serious. The gentleman in the middlecherry had stopped counting and now\n",
            "sat uprightly to one side. His expressionless body was left to its own\n",
            "turn. Now he looked uncertainly round the living room, made a movement that\n",
            "himself would follow and lean forward a little to one side. IANAM\n",
            "SAH: “What’s wrong?”, asked the middle gentleman from the\n",
            "front desk, looking over his shoulder worriedly. THE EMAIL\n",
            "SEGUTOR: Mr. Samsa, I have some serious\n",
            "uniform-swearing to do. I’m going to the office.” Mr. Samsa made a\n",
            "steadfast effort to get the door of the room he was in, knocking very\n",
            "very hard, and could be seen entering through the window through which he had\n",
            "come. The gentleman in the middle, who had not been paying attention,\n",
            "jumped off his chair and tried to get the door open. He was\n",
            "slowly making his way back through the junk, but was strongly urged\n",
            "by his wife to do so. In the darkness of his room he could see onto the\n",
            "coffee pot, the kitchen, the bedroom, the bathroom. He finally managed to\n",
            "open the door and enter through the window, but broke his concentration as\n",
            "he saw from the ceiling that he could not get out of his chair as he fell down\n",
            "into the darkness. He made a sudden movement, drew his foot vigorously\n",
            "from the darkness and ran into the doorway. The stranger was not far away from\n",
            "him, in his run-in with the law mood, he was convinced that he had\n",
            "to get out of his way so that he could as easily be struck down by the\n",
            "trembling door as by the threat of violence. The door was\n",
            "opened very slightly and Mr. Samsa was led out of the room into the\n",
            "middle of the street.\n"
          ]
        }
      ],
      "source": [
        "# This line is necessary to be able to run a new tf session if one has already been run\n",
        "tf.reset_default_graph()\n",
        "# Start a session\n",
        "sess = gpt2.start_tf_sess()\n",
        "# Fine tune `model_name` on `data`\n",
        "###################################\n",
        "# Swap out the `dataset` parameter with the path to your text dataset\n",
        "###################################\n",
        "gpt2.finetune(sess,\n",
        "              dataset='../data/text/5200-0.txt',\n",
        "              model_name=model_name,\n",
        "              restore_from='latest',\n",
        "              steps=500)   # steps is max number of training steps\n",
        "\n",
        "gpt2.generate(sess, run_name='run1')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "lab9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4f62d8fdf00849b793d430b3abdd56cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99c9608a5d9d40c2a9405a225f192276",
              "IPY_MODEL_0018ecf6e3944c849448eed708ecd33a",
              "IPY_MODEL_350e3f62f3694e4d9869b3fb4b05ad30"
            ],
            "layout": "IPY_MODEL_f6b7662ab3a046839ff4499406111951"
          }
        },
        "99c9608a5d9d40c2a9405a225f192276": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_527dcc8dd61a48ae8a07fb38508eef41",
            "placeholder": "​",
            "style": "IPY_MODEL_71f5a369b5da47af890276f5a8d9fa3b",
            "value": "100%"
          }
        },
        "0018ecf6e3944c849448eed708ecd33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a0faee4971d4a50a5c859cf719f7269",
            "max": 241627721,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ada8ef9387a740bdb22e46acb3b2647e",
            "value": 241627721
          }
        },
        "350e3f62f3694e4d9869b3fb4b05ad30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44af057aaf5245ed8848f2364e916049",
            "placeholder": "​",
            "style": "IPY_MODEL_f6f5cb77c4504f2f89b7e2333395c28b",
            "value": " 230M/230M [00:14&lt;00:00, 10.8MB/s]"
          }
        },
        "f6b7662ab3a046839ff4499406111951": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "527dcc8dd61a48ae8a07fb38508eef41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71f5a369b5da47af890276f5a8d9fa3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a0faee4971d4a50a5c859cf719f7269": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ada8ef9387a740bdb22e46acb3b2647e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44af057aaf5245ed8848f2364e916049": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6f5cb77c4504f2f89b7e2333395c28b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3735ee1f1454edca87121ad2e7eda75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_028e8c2bfc1b4c118634e62625601281",
              "IPY_MODEL_e6b7cd670e574bd38b29ae4056fcf56d",
              "IPY_MODEL_860f7787a5214a8c8c92ac581057c80b"
            ],
            "layout": "IPY_MODEL_38e612eed865438abf8c74e7351809d1"
          }
        },
        "028e8c2bfc1b4c118634e62625601281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e57071f10ffb4b60afe794a3b74a01c9",
            "placeholder": "​",
            "style": "IPY_MODEL_ea0fddb5e4184dfb9cf70922226abd54",
            "value": ""
          }
        },
        "e6b7cd670e574bd38b29ae4056fcf56d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b48f51dc73f422a8b9c675716d06ca5",
            "max": 51498,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d04fc0fcb9a242ea8355c6c8700c96b2",
            "value": 51498
          }
        },
        "860f7787a5214a8c8c92ac581057c80b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1276c8b9ab7466abde92ae10e422e22",
            "placeholder": "​",
            "style": "IPY_MODEL_a593cac4322f4015a8095a6d04c5bd36",
            "value": " 52224/? [00:00&lt;00:00, 1192364.56it/s]"
          }
        },
        "38e612eed865438abf8c74e7351809d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e57071f10ffb4b60afe794a3b74a01c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea0fddb5e4184dfb9cf70922226abd54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b48f51dc73f422a8b9c675716d06ca5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d04fc0fcb9a242ea8355c6c8700c96b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1276c8b9ab7466abde92ae10e422e22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a593cac4322f4015a8095a6d04c5bd36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}